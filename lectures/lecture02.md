### Lecture: 2. Linear Regression, SGD
#### Date: Oct 6, Oct 9
#### Slides: https://ufal.mff.cuni.cz/~courses/npfl129/2526/slides/?02
#### Reading: https://ufal.mff.cuni.cz/~courses/npfl129/2526/slides.pdf/npfl129-2526-02.pdf, PDF Slides
#### Questions: #lecture_2_questions

**Learning objectives.** After the lecture you should be able to

- Reason about **overfitting** in terms of **model capacity**.
- Use **$L^2$-regularization** to control model capacity.
- Explain what the difference between **parameters and hyperparameters** is.
- Tell what the **basic probability concepts** are (joint, marginal, conditional probability; expected value, mean, variance).
- Mathematically describe and implement the **stochastic gradient descent** algorithm.
- Use both **numerical and categorical features** in linear regression.

**Covered topics** and where to find more:

- L2 regularization in linear regression [Section 1.1, 3.1.4 of PRML]
- Random variables and probability distributions [Section 1.2, 1.2.1 of PRML]
- Expectation and variance [Section 1.2.2 of PRML]
- Gradient descent [Section 5.2.4 of PRML]
  - Stochastic gradient descent solution of linear regression
- [Linear regression demo](https://mlu-explain.github.io/linear-regression) by Jared Willber
- [Why Momentum Really Works](https://distill.pub/2017/momentum/) by Gabriel Goh
- [IPython notebook on momentum](https://github.com/ufal/npfl129/blob/master/notebooks/gradient.ipynb)